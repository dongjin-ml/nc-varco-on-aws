{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3436b526-115b-49df-a9b5-358a4e2f0d58",
   "metadata": {},
   "source": [
    "# Post Call Analytics\n",
    "Welcome to this training module on post-call analytics use cases using Amazon SageMaker JumpStart.\n",
    "\n",
    "As businesses continue to interact with customers through various channels, it becomes increasingly important to analyze these interactions to gain insights into customer behavior and preferences. Post-call analytics is one such method that involves analyzing customer interactions after the call has ended. The use of large language models can greatly enhance the effectiveness of post-call analytics by enabling more accurate sentiment analysis, identifying specific customer needs and preferences, and improving overall customer experience.\n",
    "\n",
    "In this sample notebook, we will explore following topics to demonstrate the various benefits of using Bedrock for post-call analytics and businesses gain a competitive edge in the modern marketplace.\n",
    "\n",
    "* One model handling multiple PCA tasks <BR>\n",
    "* Handling long call transcripts <BR>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3de5f9-285e-49de-9c9d-4545a3ae5cec",
   "metadata": {},
   "source": [
    "## Step 0. Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8440b9-f1e6-41de-92b4-5fc265e095ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "install_needed = True  # should only be True once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ee8812-21b3-4eea-adc2-c043d202aa2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import IPython\n",
    "\n",
    "if install_needed:\n",
    "    print(\"installing deps and restarting kernel\")\n",
    "    !{sys.executable} -m pip install -U pip\n",
    "    !{sys.executable} -m pip install -U termcolor\n",
    "    !{sys.executable} -m pip install -U langchain\n",
    "    !{sys.executable} -m pip install -U transformers\n",
    "    \n",
    "    IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396379a6-bc29-4ec1-a919-a61480a56750",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 1. SageMaker Endpoint Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1555b9-f4a5-40d6-9a3c-847edf485e8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "from pprint import pprint\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler, SagemakerEndpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03941344-f17a-44d3-9e22-97de3d140e23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VarcoContentHandler(LLMContentHandler):\n",
    "    \n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs={}) -> bytes:\n",
    "        '''\n",
    "        입력 데이터 전처리 후에 리턴\n",
    "        '''\n",
    "        \n",
    "        payload = {\"text\": prompt}\n",
    "        payload.update(model_kwargs)\n",
    "        input_str = json.dumps(payload)\n",
    "\n",
    "        return input_str.encode('utf-8')\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "    \n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))     \n",
    "        generated_text = response_json[\"result\"]        \n",
    "\n",
    "        return generated_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046eabaa-10e4-4474-a741-8f26fb53d035",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aws_region = boto3.Session().region_name\n",
    "llm_text_content_handler = VarcoContentHandler()\n",
    "endpoint_name_text = \"varco-llm-13b-ist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5bae30-528c-4e5d-bb6e-315c5a04890c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"request_output_len\": 256,\n",
    "    \"repetition_penalty\": 1.15,\n",
    "    \"temperature\": 0.1,\n",
    "    \"top_k\": 50,\n",
    "    \"top_p\": 1\n",
    "}\n",
    "\n",
    "llm_text = SagemakerEndpoint(\n",
    "    endpoint_name=endpoint_name_text,\n",
    "    region_name=aws_region,\n",
    "    model_kwargs=params,\n",
    "    content_handler=llm_text_content_handler,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db7524d-4dc8-4944-96d0-86373c6e2cd1",
   "metadata": {},
   "source": [
    "## Step 2. Load transcript files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e534aadf-217b-4d9b-ac0c-1899b08a5125",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "transcript_files = [\n",
    "    \"./call_transcripts/negative-refund-ko.txt\",\n",
    "    \"./call_transcripts/neutral-short-ko.txt\",\n",
    "    \"./call_transcripts/positive-partial-refund-ko.txt\",\n",
    "    \"./call_transcripts/aws-short-ko.txt\",\n",
    "]\n",
    "transcripts = []\n",
    "\n",
    "for file_name in transcript_files:\n",
    "    with open(file_name, \"r\") as file:\n",
    "        transcripts.append(file.read())\n",
    "\n",
    "\n",
    "for i, trans in enumerate(transcripts):\n",
    "    print(f\"transcript #{i+1}: {trans[:300]}\\n\")\n",
    "    print(\"====================\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee5590f-f417-46c2-a999-03dac5ebe115",
   "metadata": {},
   "source": [
    "## Step 3. Post Call Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496b2145-108b-4324-8dca-5707df2adbae",
   "metadata": {},
   "source": [
    "### Step 3.1. Prompt Template\n",
    "In this notebook, we'll be performing four different analyses(Summary, Sentiment, Intent and Resolution), and we'll need a template for each one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1105f04-6497-4405-aa34-0da5a5ff4a89",
   "metadata": {},
   "source": [
    "* Summary template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bded3e78-6b0c-4799-9f34-b0a0a50a019e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summary_template = \"\"\"### User:\n",
    "다음 대화를 간단하게 요약해 주세요.\n",
    "대화: {transcript}\n",
    "\n",
    "\n",
    "### Assistant:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d53d95-b136-4b9e-80c1-98e028c8847d",
   "metadata": {},
   "source": [
    "* Sentiment template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3136c75-c2c6-4f74-a188-32ff65750790",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentiment_template = \"\"\"### User:\n",
    "감성 분석 프로그램입니다. 다음 클래스를 이용하여 고객의 감성을 분류하세요. \n",
    "[\"긍정\", \"중립\", \"부정\"]. 대화를 이 클래스 중 한 가지로 정확하게 분류합니다. \n",
    "모르거나 확실하지 않은 경우 [\"중립\"] 클래스를 사용하세요. 클래스를 만들려고 하지 마세요.\n",
    "대화: {transcript}\n",
    "\n",
    "\n",
    "### Assistant:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a8c637-6034-48a4-998e-26fdf693e6df",
   "metadata": {},
   "source": [
    "* intent template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bd8ab7-233b-4553-a381-37d68521cb67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "intent_template = \"\"\"### User:\n",
    "이것은 의도 분류 프로그램입니다. 다음 대화에서 고개의 목적은 무엇입니까? \n",
    "클래스 [\"배송_지연\", \"제품_결함\", \"계정_질문\"]. 대화를 다음 클래스 중 하나로 분류합니다. \n",
    "이 클래스 중 하나에 정확히 일치합니다. 모르는 경우 [\"UNKNOWN\"] 클래스를 사용하세요. 클래스를 만들려고 하지 마세요. \n",
    "대화: {transcript}\n",
    "\n",
    "\n",
    "### Assistant:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bafa4b6-e8e2-4770-b544-f85abacc25e9",
   "metadata": {},
   "source": [
    "### Step 3.2. Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a3e9e7-130f-4e7c-910a-7a62d77480ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from termcolor import colored\n",
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38b0f5f-a215-437c-be13-7680473b6771",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def analysis(llm, transcript, params, template=\"\", max_tokens=50):\n",
    "\n",
    "    prompt = PromptTemplate(template=template, input_variables=[\"transcript\"])\n",
    "    analysis_prompt = prompt.format(transcript=transcript)\n",
    "    llm.model_kwargs = params\n",
    "\n",
    "    print (colored(analysis_prompt, 'green'))\n",
    "\n",
    "    response = llm(analysis_prompt)\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb27f4c-1490-4048-b78f-8f71a036fc4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"request_output_len\": 256,\n",
    "    \"repetition_penalty\": 0.8,\n",
    "    \"temperature\": 0.9,\n",
    "    \"top_k\": 50,\n",
    "    \"top_p\": 1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2703e3-15d0-45a6-904f-7ef5dca3f750",
   "metadata": {},
   "source": [
    "* summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883ee77f-4831-4457-aed9-0d52e606c8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "res = analysis(\n",
    "    llm=llm_text,\n",
    "    transcript=transcripts[0],\n",
    "    params=params,\n",
    "    template=summary_template\n",
    ")\n",
    "\n",
    "print (res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6913aa-bec2-4e25-b744-7386cecc7271",
   "metadata": {},
   "source": [
    "* sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf95670-5947-4163-8e79-7838e9e784f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "res = analysis(\n",
    "    llm=llm_text,\n",
    "    transcript=transcripts[0],\n",
    "    params=params,\n",
    "    template=sentiment_template\n",
    ")\n",
    "\n",
    "print (res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd66bbfc-6167-4d1c-92a5-e9fb5c8f3d14",
   "metadata": {},
   "source": [
    "* intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce8075c-1f43-4494-8ab5-bb637c85b861",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "res = analysis(\n",
    "    llm=llm_text,\n",
    "    transcript=transcripts[0],\n",
    "    params=params,\n",
    "    template=intent_template\n",
    ")\n",
    "\n",
    "print (res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038a5c45-7f0f-464b-8137-fc6ba12eacf2",
   "metadata": {},
   "source": [
    "## Handling long call transcripts\n",
    "We'll cover how to handle long transcripts that exceed the limits of the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5622dd86-0d5e-42d0-a20a-b55a4f3d393f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed960e5f-c045-43dc-85dc-e9a1e0f0c52d",
   "metadata": {},
   "source": [
    "* prompting to divide and conquer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0ac803-2753-401f-a73e-5965f8d2b26c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stuff_prompt_template = \"\"\"### User:\n",
    "다음 글을 간단하게 요약해 주세요.\n",
    "글: {text}\n",
    "\n",
    "### Assistant:\n",
    "\"\"\"\n",
    "\n",
    "chuck_prompt_template = \"\"\"### User:\n",
    "다음 글을 간단하게 요약해 주세요.\n",
    "글: {text}\n",
    "\n",
    "### Assistant:\n",
    "\"\"\"\n",
    "\n",
    "chunk_prompt = PromptTemplate(\n",
    "    template=chuck_prompt_template,\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "combine_prompt_template = \"\"\"### User:\n",
    "다음 글을 간단하게 요약해 주세요.\n",
    "글: {text}\n",
    "\n",
    "\n",
    "### Assistant:\n",
    "\"\"\"\n",
    "\n",
    "combine_prompt = PromptTemplate(\n",
    "    template=combine_prompt_template,\n",
    "    input_variables=[\"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8c1654-fc64-4fdb-88aa-433ed0266cfa",
   "metadata": {},
   "source": [
    "* summarize chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1efd25-af00-4486-abeb-8ee5a92489d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# summary_chain = load_summarize_chain(\n",
    "#     llm=llm,\n",
    "#     chain_type=\"map_reduce\",\n",
    "#     verbose=True\n",
    "# ) # map_reduce, refine\n",
    "# transcript = summary_chain(docs)\n",
    "'''\n",
    "\n",
    "\n",
    "def summary_chain_init(chain_type, llm):\n",
    "    \n",
    "    if chain_type == \"STUFF\":\n",
    "        chain = load_summarize_chain(\n",
    "            llm,\n",
    "            chain_type=\"stuff\",\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "    elif chain_type == \"MAP_REDUCE\":\n",
    "        chain = load_summarize_chain(\n",
    "            llm,\n",
    "            chain_type=\"map_reduce\",\n",
    "            map_prompt=chunk_prompt,\n",
    "            combine_prompt=combine_prompt,\n",
    "            return_intermediate_steps=True,\n",
    "            verbose=True\n",
    "        )\n",
    "    elif chain_type == \"REFINE\":\n",
    "        chain = load_summarize_chain(\n",
    "            llm,\n",
    "            chain_type=\"refine\",\n",
    "            question_prompt=chunk_prompt,\n",
    "            refine_prompt=combine_prompt,\n",
    "            return_intermediate_steps=True,\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "    return chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03fb6de-642c-41fa-9da0-a12becf9b84a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def long_call_analysis(llm, transcript, params, template=\"\", chain_type=\"MAP_REDUCE\", max_tokens=50):\n",
    "\n",
    "    \n",
    "    llm.model_kwargs = params\n",
    "    num_tokens = llm.get_num_tokens(transcript) #raise warnning\n",
    "\n",
    "    if num_tokens > max_tokens:\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            separators=[\"\\n\\n\\n\"],\n",
    "            chunk_size=500,\n",
    "            chunk_overlap=100\n",
    "        )\n",
    "        docs = text_splitter.create_documents([transcript])\n",
    "        num_docs = len(docs)\n",
    "        num_tokens_first_doc = llm.get_num_tokens(docs[0].page_content)\n",
    "\n",
    "        print(f\"Now we have {num_docs} documents and the first one has {num_tokens_first_doc} tokens\")\n",
    "\n",
    "        \n",
    "        summary_chain = summary_chain_init(\n",
    "            chain_type=chain_type, \n",
    "            llm=llm\n",
    "        )\n",
    "        response = summary_chain(\n",
    "            {\"input_documents\": docs}\n",
    "        )\n",
    "        \n",
    "        print (\"Intermediate_steps: \\n\")\n",
    "        for idx, step in enumerate(response[\"intermediate_steps\"]):\n",
    "            print (colored(f'step {idx}: \\n', \"green\"))\n",
    "            print (colored(f'{step}\\n', \"green\"))\n",
    "        \n",
    "        return response[\"output_text\"]\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        prompt = PromptTemplate(template=stuff_prompt_template, input_variables=[\"text\"])\n",
    "        analysis_prompt = prompt.format(text=transcript)\n",
    "        print (colored(analysis_prompt, 'green'))\n",
    "        \n",
    "        response = llm(analysis_prompt)\n",
    "        \n",
    "        return response\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc7863c-5190-4f71-89a2-bb92c6f845fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"repetition_penalty\": 1.01,\n",
    "    \"temperature\": 0.1,\n",
    "    \"top_k\": 50,\n",
    "    \"top_p\": 0.9\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372ce0b2-00af-46de-b035-9fdf00b0eab1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "%%time\n",
    "\n",
    "res = long_call_analysis(\n",
    "    llm=llm_text,\n",
    "    transcript=transcripts[3],\n",
    "    params=params,\n",
    "    template=summary_template,\n",
    "    chain_type=\"REFINE\" # REFINE, MAP_REDUCE\n",
    ")\n",
    "\n",
    "print (\"Results: \\n\")\n",
    "print (res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406f11e8-3b9f-4408-be5c-a3ada85604ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
